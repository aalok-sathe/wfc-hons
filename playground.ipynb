{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: transformers 4.0.0 on mcsgpu\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wiki_fact_check/plain_text (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /sw/mcs/wfc/datasets/wiki_fact_check/plain_text/1.0.0/7158b728cbd5e133282afd03da45069799a4cfaed9966aef4df51ee156d0d2fa...\n",
      "INFO: {'train': '/sw/mcs/wfc/datasets/downloads/737bc15ea4440e012d868c6f61efb6f4b91e4176bc22853a1bb864d9c2448022', 'dev': '/sw/mcs/wfc/datasets/downloads/7a674645a64b0dea3de87f545b162a0d8f60d2396ad75ff3203a76259f84c52f'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d85967d834e45d099a1792a45230097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5cb8cf47d543a8b90d8f5b3cbb61e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO: encountered 20 errors processing /sw/mcs/wfc/datasets/downloads/737bc15ea4440e012d868c6f61efb6f4b91e4176bc22853a1bb864d9c2448022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db884b671c84d52a7c06d6ae405061a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99bf154bc724ee7a09e48e0534e8d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO: encountered 5 errors processing /sw/mcs/wfc/datasets/downloads/7a674645a64b0dea3de87f545b162a0d8f60d2396ad75ff3203a76259f84c52f\n",
      "Dataset wiki_fact_check downloaded and prepared to /sw/mcs/wfc/datasets/wiki_fact_check/plain_text/1.0.0/7158b728cbd5e133282afd03da45069799a4cfaed9966aef4df51ee156d0d2fa. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# dataset = datasets.load_dataset('./data/wikifactcheck', split='train',\n",
    "#                                 cache_dir='/sw/mcs/wfc/datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'claim', 'context', 'evidence', 'label'],\n",
       "    num_rows: 38916\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk('/sw/mcs/wfc/datasets/wfc_train_with_evidence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wiki_fact_check (/sw/mcs/wfc/datasets/wiki_fact_check/plain_text/1.0.0/7158b728cbd5e133282afd03da45069799a4cfaed9966aef4df51ee156d0d2fa)\n"
     ]
    }
   ],
   "source": [
    "# dataset_dev = datasets.load_dataset('./data/wikifactcheck', split='validation',\n",
    "#                                     cache_dir='/sw/mcs/wfc/datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'claim', 'context', 'evidence', 'label'],\n",
       "    num_rows: 9730\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dev.save_to_disk('/sw/mcs/wfc/datasets/wfc_dev_with_evidence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now try to train a BERT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: transformers 4.0.0 on mcsgpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_from_disk('/sw/mcs/wfc/datasets/wfc_train_with_evidence')\n",
    "dataset_dev = datasets.load_from_disk('/sw/mcs/wfc/datasets/wfc_dev_with_evidence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/distilbert-base-nli-stsb-mean-tokens', \n",
    "                                          max_length=128)\n",
    "\n",
    "def tokenize_wfc(e, tokenizer=tokenizer):\n",
    "    cl = tokenizer(e['claim'], truncation=True, padding='max_length')\n",
    "    ev = map(lambda sent: tokenizer(sent, truncation=True, padding='max_length'),\n",
    "             e['evidence'].split('. '))\n",
    "    return {\n",
    "        **cl,\n",
    "        'evidence': list(ev)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-df08e5580369>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_tok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_wfc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataset_dev_tok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_wfc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_tok = dataset.map(tokenize_wfc, batched=False)\n",
    "dataset_dev_tok = dataset_dev.map(tokenize_wfc, batched=False)\n",
    "\n",
    "# dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "The :class:`~transformers.DistilBertModel` forward method, overrides the :func:`__call__` special method.\n",
       "\n",
       ".. note::\n",
       "    Although the recipe for forward pass needs to be defined within this function, one should call the\n",
       "    :class:`Module` instance afterwards instead of this since the former takes care of running the pre and post\n",
       "    processing steps while the latter silently ignores them.\n",
       "    \n",
       "Args:\n",
       "    input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, num_choices)`):\n",
       "        Indices of input sequence tokens in the vocabulary.\n",
       "\n",
       "        Indices can be obtained using :class:`~transformers.DistilBertTokenizer`. See\n",
       "        :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n",
       "        details.\n",
       "\n",
       "        `What are input IDs? <../glossary.html#input-ids>`__\n",
       "    attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, num_choices)`, `optional`):\n",
       "        Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n",
       "\n",
       "        - 1 for tokens that are **not masked**,\n",
       "        - 0 for tokens that are **masked**.\n",
       "\n",
       "        `What are attention masks? <../glossary.html#attention-mask>`__\n",
       "    head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\n",
       "        Mask to nullify selected heads of the self-attention modules. Mask values selected in ``[0, 1]``:\n",
       "\n",
       "        - 1 indicates the head is **not masked**,\n",
       "        - 0 indicates the head is **masked**.\n",
       "\n",
       "    inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, num_choices, hidden_size)`, `optional`):\n",
       "        Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\n",
       "        This is useful if you want more control over how to convert :obj:`input_ids` indices into associated\n",
       "        vectors than the model's internal embedding lookup matrix.\n",
       "    output_attentions (:obj:`bool`, `optional`):\n",
       "        Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n",
       "        tensors for more detail.\n",
       "    output_hidden_states (:obj:`bool`, `optional`):\n",
       "        Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n",
       "        more detail.\n",
       "    return_dict (:obj:`bool`, `optional`):\n",
       "        Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> from transformers import DistilBertTokenizer, DistilBertModel\n",
       "    >>> import torch\n",
       "\n",
       "    >>> tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
       "    >>> model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
       "\n",
       "    >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
       "    >>> outputs = model(**inputs)\n",
       "\n",
       "    >>> last_hidden_states = outputs.last_hidden_state\n",
       "\n",
       "Returns:\n",
       "    :class:`~transformers.modeling_outputs.BaseModelOutput` or :obj:`tuple(torch.FloatTensor)`: A :class:`~transformers.modeling_outputs.BaseModelOutput` (if\n",
       "    ``return_dict=True`` is passed or when ``config.return_dict=True``) or a tuple of :obj:`torch.FloatTensor`\n",
       "    comprising various elements depending on the configuration (:class:`~transformers.DistilBertConfig`) and inputs.\n",
       "\n",
       "    - **last_hidden_state** (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`) -- Sequence of hidden-states at the output of the last layer of the model.\n",
       "    - **hidden_states** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``) -- Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
       "      of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
       "\n",
       "      Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
       "    - **attentions** (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``) -- Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
       "      sequence_length, sequence_length)`.\n",
       "\n",
       "      Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
       "      heads.\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> from transformers import DistilBertTokenizer, DistilBertModel\n",
       "    >>> import torch\n",
       "\n",
       "    >>> tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
       "    >>> model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
       "\n",
       "    >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
       "    >>> outputs = model(**inputs)\n",
       "\n",
       "    >>> last_hidden_states = outputs.last_hidden_state\n",
       "\u001b[0;31mFile:\u001b[0m      ~/code/wfc-hons/transformers/models/distilbert/modeling_distilbert.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoModelForSequenceClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained('./wfclm/bert_5000_lr=1e-05/checkpoint-150000', max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/as9kc/code/wfc-hons/transformers/models/auto/modeling_auto.py:852: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "lm = AutoModelWithLMHead.from_pretrained('./wfclm/bert_5000_lr=1e-05/checkpoint-150000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tok.batch_encode_plus([f'The hindwings are flat with a {tok.mask_token} line across the dorsal'], \n",
    "                            padding='max_length', truncation=True, max_length=128)\n",
    "enc = {k: torch.Tensor(v).long() for k,v in enc.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmout = lm(**enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. the hindwings are grey with a dark line of the dorsal.................... a.. a a... a..................... a.................................................................'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode(torch.argmax(lmout.logits, dim=2).view(-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlu)",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
